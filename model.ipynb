{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":997012,"sourceType":"datasetVersion","datasetId":546691},{"sourceId":7582110,"sourceType":"datasetVersion","datasetId":4413702},{"sourceId":9506165,"sourceType":"datasetVersion","datasetId":4260500}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nimport torch.utils as utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport os\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self):\n        self.t = transforms.Compose([transforms.ToTensor(),transforms.CenterCrop((350,350)),transforms.Resize((150,150))])\n        self.data = os.listdir(img_dir)\n        self.trainp = 0.9\n    def __len__(self):\n        return int(len(self.data)*0.9)\n    def __getitem__(self,i):\n        img = self.t(mimg.imread(img_dir+self.data[i]))\n        return img\nd = dataset()\n#t = transforms.Compose([transforms.ToTensor(),transforms.CenterCrop((250,250)),transforms.Resize((150,150))])\nt = transforms.Compose([transforms.ToTensor(),transforms.Resize((150,150))])\n#d = torchvision.datasets.CIFAR10(root= \"/\", train = True, transform= t, download= True)\nloader = DataLoader(d,20)\ntesta = next(iter(loader))\ntestimg = testa[4]\nplt.imshow(torch.permute(testimg,(1,2,0)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IH = 32\nIW = 32\nc = 8\n#^","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"nn.Conv2d(in_channels= 3 , out_channels= 6 , kernel_size= 5 , stride= 2 , padding= 0 )\nnn.Conv2d(in_channels= 6 , out_channels= 9 , kernel_size= 3 , stride= 2 , padding= 0 )\nnn.Conv2d(in_channels= 9 , out_channels= 12 , kernel_size= 3 , stride= 2 , padding= 0 )\nnn.Conv2d(in_channels= 12 , out_channels= 15 , kernel_size= 3 , stride= 1 , padding= 0 )\nnn.ConvTranspose2d(in_channels= 15 , out_channels= 12 , kernel_size= 3 , stride= 1 , padding= 0 )\nnn.ConvTranspose2d(in_channels= 12 , out_channels= 9 , kernel_size= 3 , stride= 2 , padding= 0 )\nnn.ConvTranspose2d(in_channels= 9 , out_channels= 6 , kernel_size= 3 , stride= 2 , padding= 0 )\nnn.ConvTranspose2d(in_channels= 6 , out_channels= 3 , kernel_size= 5 , stride= 2 , padding= 0 )\nnn.ConvTranspose2d(in_channels= 3 , out_channels= 3 , kernel_size= 3 , stride= 1 , padding= 0 )\nnn.ConvTranspose2d(in_channels= 3 , out_channels= 3 , kernel_size= 4 , stride= 1 , padding= 0 )\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class nblock(nn.Module):\n    def __init__(self,c,encoder:bool,k, s,an ):\n        self.helper = [c[0]]\n        super(nblock, self).__init__()\n        self.enc = encoder\n        self.an = an\n        self.c = c\n        self.k = k\n        self.s = s\n        layers = []\n        skips = []\n        for i in range(0,len(k)):\n            l4 = self.makel(i)\n            layers.append(l4[0])\n            if((i+1)%an==0):\n                skips.append(l4[1])\n        self.layers = nn.ModuleList(layers)\n        self.skips = nn.ModuleList(skips)\n    def makel(self,i):\n        if (i+1)%self.an==0:\n            rv =  (nn.Sequential(nn.ConvTranspose2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]),nn.LeakyReLU()),nn.Conv2d(self.helper[-1],self.c[i+1],1,1),) if self.enc ==False else (nn.Sequential(nn.Conv2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]),nn.LeakyReLU()),nn.Conv2d(self.helper[-1],self.c[i+1],1,1))\n            self.helper.append(self.c[i+1])\n        else:\n            return(nn.Sequential(nn.ConvTranspose2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]))) if self.enc == False else nn.Sequential(nn.Conv2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]),)\n        return rv\n\n    def forward(self,x):\n        x0=[x.clone()]\n        y=x\n#        for i in range(len(self.layers)):\n#            y = self.layers[i][0](y)\n#            if((i+1))%self.an==0:\n#                y = torch.add(y,self.layers[i][1](transforms.Resize(y.shape[-2:])(x0)))\n#                x0 = y\n        n=0\n        for i, l in enumerate(self.layers):\n            y = l(y)\n            if(i+1)%self.an==0:\n                #y = torch.add(y,self.skips[n](transforms.Resize(y.shape[-2:])(x0[-1])))\n                #x0.append(y.clone())\n                n+=1\n        return y\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class rblock(nn.Module):\n    def __init__(self,c,encoder:bool,k, s,an ):\n        self.helper = [c[0]]\n        super(rblock, self).__init__()\n        self.enc = encoder\n        self.an = an\n        self.c = c\n        self.k = k\n        self.s = s\n        layers = []\n        skips = []\n        for i in range(0,len(k)):\n            l4 = self.makel(i)\n            layers.append(l4[0])\n            if((i+1)%an==0):\n                skips.append(l4[1])\n        self.layers = nn.ModuleList(layers)\n        self.skips = nn.ModuleList(skips)\n    def makel(self,i):\n        if (i+1)%self.an==0:\n            rv =  (nn.Sequential(nn.ConvTranspose2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]),nn.LeakyReLU()),nn.Conv2d(self.helper[-1],self.c[i+1],1,1)) if self.enc ==False else (nn.Sequential(nn.Conv2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]),nn.LeakyReLU()),nn.Conv2d(self.helper[-1],self.c[i+1],1,1))\n            self.helper.append(self.c[i+1])\n        else:\n            return(nn.Sequential(nn.ConvTranspose2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]))) if self.enc == False else nn.Sequential(nn.Conv2d(in_channels=self.c[i],out_channels= self.c[i+1] ,kernel_size= self.k[i],stride= self.s[i]))\n        return rv\n\n    def forward(self,x):\n        x0=[x.clone()]\n        y=x\n#        for i in range(len(self.layers)):\n#            y = self.layers[i][0](y)\n#            if((i+1))%self.an==0:\n#                y = torch.add(y,self.layers[i][1](transforms.Resize(y.shape[-2:])(x0)))\n#                x0 = y\n        n=0\n        for i, l in enumerate(self.layers):\n            y = l(y)\n            if(i+1)%self.an==0:\n                y = torch.add(y,self.skips[n](transforms.Resize(y.shape[-2:])(x0[-1])))\n                x0.append(y.clone())\n                n+=1\n        return y\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.net = nblock((6,3,3,2,2,1,1),True,(3,3,3,3,3,3),(2,2,1,1,1,1),20)\n        self.lin = nn.Sequential(nn.Linear(784,500),nn.Linear(500,100),nn.Linear(100,1))\n    def forward(self, x):\n        return nn.Sigmoid()(self.lin(nn.Flatten()(self.net(x))))\n\nDisc = Discriminator().to(device)\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.net2 = nblock((3,9,12,15,15),True,(3,3,3,3),(2,2,2,2),2)\n        self.net = rblock((15,15,12,9,3,3),False,(3,3,3,3,8),(2,2,2,2,1),2)\n    def forward(self, x):\n        x2 = self.net2(x)\n        return self.net(x2)\nGen = Decoder().to(device)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Disc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Gen.train()\nDisc.train()\n\nlr = 0.001\ngOpt = torch.optim.Adam(Gen.parameters(), lr = 0.0002)\ndOpt = torch.optim.Adam(Disc.parameters(), lr =0.0002)\nmae = nn.L1Loss()\nbce = nn.BCELoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(0,10):\n    for l in enumerate(tqdm(loader)):\n        Disc.zero_grad()\n        im = l[1].to(device)\n        recon = Gen(im)\n        doutr = Disc(torch.cat((im,im),1))\n        dlossr = bce(doutr,torch.ones_like(doutr))\n        doutf = Disc(torch.cat((recon,im),1))\n        dlossf = bce(doutf,torch.zeros_like(doutf))\n        dlossr.backward(retain_graph = True)\n        dlossf.backward(retain_graph = True)\n        dOpt.step()\n        \n        Gen.zero_grad()\n        recon2= Gen(im)\n        doutf2 = Disc(torch.cat((recon2,im),1))\n        gloss = bce(doutf2,torch.ones_like(doutf2))+(4*mae(recon,im))\n        gloss.backward(retain_graph = True)\n        gOpt.step()\n\n        if(l[0]%100==0):\n            f, axarr = plt.subplots(1,2) \n            axarr[0].imshow(torch.permute(Gen(testa.to(device)).to(\"cpu\").detach()[4],(1,2,0)))\n            axarr[1].imshow(torch.permute(testa[4],(1,2,0)).detach().to(\"cpu\"))\n            plt.show()\n            print(dlossr,gloss)\n        if(epoch%3==0):\n            torch.save(Gen.state_dict(), \"gen\"+str(epoch))\n            torch.save(Disc.state_dict(), \"disc\"+str(epoch))\n        torch.cuda.empty_cache()\n\n\n        \n\n        recon3 = Gen(im)\n        doutf3 = Disc(torch.cat((recon3,im),1))\n        eloss = bce(doutf3,torch.zeros_like(doutf3)) +(4*mae(im,recon3))\n        del im\n        del recon\n        del doutf\n        del doutr\n        eloss.backward()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(0,10):\n    for l in enumerate(tqdm(loader)):\n        Disc.zero_grad()\n        im = l[1].to(device)\n        recon = Gen(im)\n        #print(recon.shape,im.shape)\n        doutr = Disc(torch.cat((im,im),1))\n        dlossr = bce(doutr,torch.ones_like(doutr))\n        doutf = Disc(torch.cat((recon,im),1))\n        dlossf = bce(doutf,torch.zeros_like(doutf))\n        dlossr.backward(retain_graph = True)\n        dlossf.backward(retain_graph = True)\n        dOpt.step()\n        \n        Gen.zero_grad()\n        recon2= Gen(im)\n        doutf2 = Disc(torch.cat((recon2,im),1))\n        gloss = bce(doutf2,torch.ones_like(doutf2))+(4*mae(recon,im))\n        gloss.backward(retain_graph = True)\n        gOpt.step()\n\n        if(l[0]%1000==0):\n            f, axarr = plt.subplots(1,2) \n            axarr[0].imshow(torch.permute(Gen(testa.to(device)).to(\"cpu\").detach()[4],(1,2,0)))\n            axarr[1].imshow(torch.permute(testa[4],(1,2,0)).detach().to(\"cpu\"))\n            plt.show()\n            print(dlossr,gloss)\n        if(epoch%3==0):\n            torch.save(Gen.state_dict(), \"gen\"+str(epoch))\n            torch.save(Disc.state_dict(), \"disc\"+str(epoch))\n        torch.cuda.empty_cache()\n\n\n        \n\n        recon3 = Gen(im)\n        doutf3 = Disc(torch.cat((recon3,im),1))\n        eloss = bce(doutf3,torch.zeros_like(doutf3)) +(4*mae(im,recon3))\n        del im\n        del recon\n        del doutf\n        del doutr\n        eloss.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(Gen.state_dict(), \"Gen.model\")\ntorch.save(Disc.state_dict(), \"Disc.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset2(Dataset):\n    def __init__(self):\n        self.t = transforms.Compose([transforms.ToTensor(),transforms.CenterCrop((150,150)),transforms.Resize((150,150))])\n        self.trainp = 0.9\n    def __len__(self):\n        return 20\n    def __getitem__(self,i):\n        img = self.t(mimg.imread(\"/input/testimage/rock.jpg\"))\n        return img\nd = dataset2()\n#t = transforms.Compose([transforms.ToTensor(),transforms.CenterCrop((250,250)),transforms.Resize((150,150))])\nloader = DataLoader(d,20)\ntesta = next(iter(loader))\ntestimg = testa[0]\ntestimg = torch.permute(testimg,(1,2,0))\nplt.imshow(testimg)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed = torch.permute(Gen(torch.unsqueeze(testa[0].to(device),0)).detach().to(\"cpu\"),(0,2,3,1))[0]\nplt.imshow(reconstructed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}